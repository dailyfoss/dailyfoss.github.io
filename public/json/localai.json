{
  "name": "localai",
  "slug": "localai",
  "tagline": "A self-hosted AI inference engine for running large language models locally",
  "subtitle": "",
  "description": "LocalAI is a self-hosted artificial intelligence inference engine that runs LLMs locally with no external API calls. It supports model selection, GPU acceleration, and provides an API compatible with OpenAI.",
  "categories": [
    13
  ],
  "metadata": {
    "sponsored": false,
    "license": "MIT",
    "version": "v3.9.0",
    "date_app_added": "2025-11-23",
    "date_last_released": "2025-12-24",
    "date_last_commit": "2025-12-31",
    "github_stars": 40911,
    "github_contributors": 157,
    "github_commits_this_year": 1994,
    "github_issues_open": 0,
    "github_issues_closed_this_year": 0,
    "github_releases_this_year": 25
  },
  "resources": {
    "website": "https://localai.io",
    "documentation": "https://github.com/mudler/LocalAI",
    "source_code": "https://github.com/mudler/LocalAI",
    "logo": "/icons/localai.webp",
    "screenshot": "",
    "issues": "https://github.com/mudler/LocalAI/issues",
    "releases": "https://github.com/mudler/LocalAI/releases"
  },
  "features": [
    {
      "title": "Local AI Inference",
      "description": "Run large language models locally",
      "core_feature": true
    },
    {
      "title": "OpenAI-Compatible API",
      "description": "Provide APIs compatible with OpenAI clients",
      "core_feature": true
    },
    {
      "title": "Model Management",
      "description": "Load and switch between supported models",
      "core_feature": true
    },
    {
      "title": "Offline Operation",
      "description": "Use AI features without external services"
    },
    {
      "title": "Hardware Acceleration",
      "description": "Use available CPU or GPU resources"
    },
    {
      "title": "Self-Hosted Deployment",
      "description": "Run AI workloads on your own infrastructure"
    }
  ],
  "platform_support": {
    "desktop": {
      "linux": true,
      "windows": false,
      "macos": true
    },
    "mobile": {
      "android": false,
      "ios": false
    },
    "web_app": false,
    "browser_extension": false,
    "cli_only": false
  },
  "hosting_options": {
    "self_hosted": false,
    "managed_cloud": false,
    "saas": false
  },
  "interfaces": {
    "cli": false,
    "gui": false,
    "web_ui": false,
    "api": false,
    "tui": false
  },
  "deployment_methods": {
    "script": true,
    "docker": true,
    "docker_compose": true,
    "helm": true,
    "kubernetes": true,
    "terraform": true
  },
  "manifests": {},
  "default_credentials": {
    "username": null,
    "password": null
  },
  "notes": [],
  "repository_status": null,
  "community_integrations": {
    "yunohost": {
      "supported": true,
      "repo_name": "localai_ynh",
      "url": "https://github.com/YunoHost-Apps/localai_ynh"
    },
    "truenas": {
      "supported": true,
      "app_name": "localai",
      "url": "https://apps.truenas.com/catalog/localai"
    }
  }
}
